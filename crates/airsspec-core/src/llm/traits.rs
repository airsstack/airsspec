//! LLM provider traits.
//!
//! This module defines the core traits that all LLM providers must implement.
//! These traits enable swapping between different LLM implementations
//! without changing application code.

// Layer 1: Standard library imports (none needed)

// Layer 2: Third-party crate imports
use async_trait::async_trait;

// Layer 3: Internal module imports
use crate::error::LlmError;
use crate::llm::types::{CompletionRequest, TokenUsage};

/// LLM provider trait.
///
/// This trait defines the interface that all LLM implementations must follow.
/// Implementations can be synchronous or asynchronous, but the API is
/// designed for async use with `tokio` or similar runtimes.
///
/// # Examples
///
/// Implementing for an `OpenAI` provider:
///
/// ```ignore
/// use async_trait::async_trait;
/// use airsspec_core::llm::{LLMProvider, types::{CompletionRequest, TokenUsage}};
/// use airsspec_core::error::LlmError;
///
/// pub struct OpenAIProvider {
///     api_key: String,
/// }
///
/// #[async_trait]
/// impl LLMProvider for OpenAIProvider {
///     async fn complete(&self, request: CompletionRequest) -> Result<String, LlmError> {
///         // Call OpenAI API and return the completion
///         todo!()
///     }
///
///     async fn complete_with_usage(
///         &self,
///         request: CompletionRequest,
///     ) -> Result<(String, TokenUsage), LlmError> {
///         // Call OpenAI API and return completion with token usage
///         todo!()
///     }
/// }
/// ```
#[async_trait]
pub trait LLMProvider: Send + Sync {
    /// Generate a completion for the given request.
    ///
    /// This method sends the request to the LLM provider and returns the
    /// generated completion as a string.
    ///
    /// # Arguments
    ///
    /// * `request` - The completion request containing messages and parameters
    ///
    /// # Returns
    ///
    /// A `Result` containing the completion text or an `LlmError`.
    ///
    /// # Errors
    ///
    /// Returns an error if:
    /// - The request is invalid
    /// - The API call fails
    /// - The response is malformed
    /// - Token usage exceeds budget
    async fn complete(&self, request: CompletionRequest) -> Result<String, LlmError>;

    /// Generate a completion with token usage statistics.
    ///
    /// This method is similar to `complete`, but also returns detailed token
    /// usage information (prompt, completion, and total tokens). Use this
    /// for cost tracking and budget management.
    ///
    /// # Arguments
    ///
    /// * `request` - The completion request containing messages and parameters
    ///
    /// # Returns
    ///
    /// A `Result` containing a tuple of (completion text, token usage) or an `LlmError`.
    ///
    /// # Errors
    ///
    /// Returns an error if:
    /// - The request is invalid
    /// - The API call fails
    /// - The response is malformed
    /// - Token usage exceeds budget
    async fn complete_with_usage(
        &self,
        request: CompletionRequest,
    ) -> Result<(String, TokenUsage), LlmError>;
}

/// Streaming response handler trait.
///
/// This trait defines the interface for handling streaming LLM responses.
/// Implementations receive tokens as they are generated, enabling real-time
/// display or processing of partial completions.
///
/// # Examples
///
/// Implementing a simple console stream handler:
///
/// ```ignore
/// use async_trait::async_trait;
/// use airsspec_core::llm::{StreamHandler, error::LlmError};
/// use std::io::{self, Write};
///
/// pub struct ConsoleStreamHandler;
///
/// #[async_trait]
/// impl StreamHandler for ConsoleStreamHandler {
///     async fn on_token(&mut self, token: &str) {
///         print!("{}", token);
///         io::stdout().flush().unwrap();
///     }
///
///     async fn on_complete(&mut self) {
///         println!(); // End the line
///     }
///
///     async fn on_error(&mut self, error: &LlmError) {
///         eprintln!("Stream error: {error}");
///     }
/// }
/// ```
#[async_trait]
pub trait StreamHandler: Send + Sync {
    /// Called when a new token is received.
    ///
    /// This method is called for each token generated by the LLM during
    /// streaming. Implementations can display, accumulate, or process tokens
    /// in real-time.
    ///
    /// # Arguments
    ///
    /// * `token` - The token text (may be partial words)
    async fn on_token(&mut self, token: &str);

    /// Called when the stream is complete.
    ///
    /// This method is called after all tokens have been received. Implementations
    /// can use this to perform cleanup or finalize accumulated output.
    async fn on_complete(&mut self);

    /// Called when an error occurs during streaming.
    ///
    /// This method is called if the stream fails for any reason (network error,
    /// API error, malformed response, etc.).
    ///
    /// # Arguments
    ///
    /// * `error` - The error that occurred
    async fn on_error(&mut self, error: &LlmError);
}

#[cfg(test)]
mod tests {
    use super::*;

    /// Mock implementation of `LLMProvider` for testing.
    struct MockLlmProvider;

    #[async_trait]
    impl LLMProvider for MockLlmProvider {
        async fn complete(&self, request: CompletionRequest) -> Result<String, LlmError> {
            Ok(format!(
                "Mock completion for {} messages",
                request.messages.len()
            ))
        }

        async fn complete_with_usage(
            &self,
            request: CompletionRequest,
        ) -> Result<(String, TokenUsage), LlmError> {
            let text = format!("Mock completion for {} messages", request.messages.len());
            let usage = TokenUsage::new(10, 5);
            Ok((text, usage))
        }
    }

    /// Mock implementation of `StreamHandler` for testing.
    struct MockStreamHandler {
        tokens: Vec<String>,
        completed: bool,
        error_received: bool,
    }

    impl MockStreamHandler {
        fn new() -> Self {
            Self {
                tokens: Vec::new(),
                completed: false,
                error_received: false,
            }
        }
    }

    #[async_trait]
    impl StreamHandler for MockStreamHandler {
        async fn on_token(&mut self, token: &str) {
            self.tokens.push(token.to_string());
        }

        async fn on_complete(&mut self) {
            self.completed = true;
        }

        async fn on_error(&mut self, _error: &LlmError) {
            self.error_received = true;
        }
    }

    #[tokio::test]
    async fn test_llm_provider_complete() {
        let provider = MockLlmProvider;
        let request = CompletionRequest {
            messages: vec![],
            max_tokens: None,
            temperature: None,
        };

        let result = provider.complete(request).await;
        assert!(result.is_ok());
        match result {
            Ok(text) => assert_eq!(text, "Mock completion for 0 messages"),
            Err(e) => panic!("Expected success, got error: {e}"),
        }
    }

    #[tokio::test]
    async fn test_llm_provider_complete_with_usage() {
        let provider = MockLlmProvider;
        let request = CompletionRequest {
            messages: vec![],
            max_tokens: None,
            temperature: None,
        };

        let result = provider.complete_with_usage(request).await;
        assert!(result.is_ok());

        match result {
            Ok((text, usage)) => {
                assert_eq!(text, "Mock completion for 0 messages");
                assert_eq!(usage.prompt_tokens, 10);
                assert_eq!(usage.completion_tokens, 5);
                assert_eq!(usage.total_tokens, 15);
            }
            Err(e) => panic!("Expected success, got error: {e}"),
        }
    }

    #[tokio::test]
    async fn test_stream_handler_on_token() {
        let mut handler = MockStreamHandler::new();
        handler.on_token("Hello").await;
        handler.on_token(" ").await;
        handler.on_token("world").await;

        assert_eq!(handler.tokens.len(), 3);
        assert_eq!(handler.tokens[0], "Hello");
        assert_eq!(handler.tokens[1], " ");
        assert_eq!(handler.tokens[2], "world");
    }

    #[tokio::test]
    async fn test_stream_handler_on_complete() {
        let mut handler = MockStreamHandler::new();
        assert!(!handler.completed);

        handler.on_complete().await;
        assert!(handler.completed);
    }

    #[tokio::test]
    async fn test_stream_handler_on_error() {
        let mut handler = MockStreamHandler::new();
        let test_error = LlmError::Request("Test error".to_string());

        handler.on_error(&test_error).await;
        assert!(handler.error_received);
    }
}
